{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7583b508173f1d2298c05ddc3913393c5338cd44"
   },
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "f286bd51cc898148944d909d75aa9e8f40109b68"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ceb127539f21b85c912b3b9266932203b4b3ed1c"
   },
   "source": [
    "## Data Handeling & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "iris = pd.read_csv(\"iris.csv\")\n",
    "iris = iris.sample(frac=1).reset_index(drop=True) # Shuffle\n",
    "#shuffled dataset is better both for separating the dataset into train/test/validation and for avoiding overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width',\n",
       "       'Species'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sepal.Length    0\n",
       "Sepal.Width     0\n",
       "Petal.Length    0\n",
       "Petal.Width     0\n",
       "Species         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the modules \n",
    "from bokeh.sampledata.iris import flowers \n",
    "from bokeh.plotting import figure, show, output_file \n",
    "\n",
    "# file to save the model \n",
    "output_file(\"gfg.html\") \n",
    "\t\n",
    "# instantiating the figure object \n",
    "graph = figure(title = \"Iris Visualization\") \n",
    "\n",
    "# labeling the x-axis and the y-axis \n",
    "graph.xaxis.axis_label = \"Petal Length (in cm)\"\n",
    "graph.yaxis.axis_label = \"Petal Width (in cm)\"\n",
    "\n",
    "# plotting for setosa petals \n",
    "x = flowers[flowers[\"species\"] == \"setosa\"][\"petal_length\"] \n",
    "y = flowers[flowers[\"species\"] == \"setosa\"][\"petal_width\"] \n",
    "color = \"blue\"\n",
    "legend_label = \"setosa petals\"\n",
    "graph.circle(x, y, \n",
    "\t\t\tcolor = color, \n",
    "\t\t\tlegend_label = legend_label) \n",
    "\n",
    "# plotting for versicolor petals \n",
    "x = flowers[flowers[\"species\"] == \"versicolor\"][\"petal_length\"] \n",
    "y = flowers[flowers[\"species\"] == \"versicolor\"][\"petal_width\"] \n",
    "color = \"yellow\"\n",
    "legend_label = \"versicolor petals\"\n",
    "graph.circle(x, y, \n",
    "\t\t\tcolor = color, \n",
    "\t\t\tlegend_label = legend_label) \n",
    "\n",
    "# plotting for virginica petals \n",
    "x = flowers[flowers[\"species\"] == \"virginica\"][\"petal_length\"] \n",
    "y = flowers[flowers[\"species\"] == \"virginica\"][\"petal_width\"] \n",
    "color = \"red\"\n",
    "legend_label = \"virginica petals\"\n",
    "graph.circle(x, y, \n",
    "\t\t\tcolor = color, \n",
    "\t\t\tlegend_label = legend_label) \n",
    "\n",
    "# relocating the legend table to \n",
    "# avoid abstruction of the graph \n",
    "graph.legend.location = \"top_left\"\n",
    "\n",
    "# displaying the model \n",
    "show(graph) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA TRAIN AND TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "0faf4527738c5324b309541c5c1b20070c1042b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.4, 2.8, 5.6, 2.2],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [4.8, 3.4, 1.9, 0.2]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris[['Sepal.Length', 'Sepal.Width', 'Petal.Length', 'Petal.Width']]\n",
    "X = np.array(X)\n",
    "X[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "a77cc8ea5693d0075fe19555c4251a97f5da6d31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "Y = iris.Species\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\n",
    "Y[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "9633cadd39b060c9a8d8c3669e36268eb37b1d89"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15)\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47c9319aab08411cfc4d4b3732f7e9d8f757b0b0"
   },
   "source": [
    "## Implementation\n",
    "\n",
    "* `X_train`, `Y_train`: The training data and target values.\n",
    "* `X_val`, `Y_val`: The validation data and target values. These are optional parameters.\n",
    "* `epochs`: Number of epochs. Defaults at 10.\n",
    "* `nodes`: A list of integers. Each integer denotes the number of nodes in each layer. The length of this list denotes the number of layers. That is, each integer in this list corresponds to the number of nodes in each layer.\n",
    "* `lr`: The learning rate of the back-propagation training algorithm. Defaults at 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "3b604e523b55665344e9da4a1e99b55bb43b94cd"
   },
   "outputs": [],
   "source": [
    "def NeuralNetwork(X_train, Y_train, X_val=None, Y_val=None, epochs=10, nodes=[], lr=0.15):\n",
    "    hidden_layers = len(nodes) - 1\n",
    "    weights = InitializeWeights(nodes)\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        weights = Train(X_train, Y_train, lr, weights)\n",
    "\n",
    "        if(epoch % 20 == 0):\n",
    "            print(\"Epoch {}\".format(epoch))\n",
    "            print(\"Training Accuracy:{}\".format(Accuracy(X_train, Y_train, weights)))\n",
    "            if X_val.any():\n",
    "                print(\"Validation Accuracy:{}\".format(Accuracy(X_val, Y_val, weights)))\n",
    "            \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "199f98ad2d9c26eef94cf1d7d167aa670393f425"
   },
   "outputs": [],
   "source": [
    "def InitializeWeights(nodes):\n",
    "    \"\"\"Initialize weights with random values in [-1, 1] (including bias)\"\"\"\n",
    "    layers, weights = len(nodes), []\n",
    "    \n",
    "    for i in range(1, layers):\n",
    "        w = [[np.random.uniform(-1, 1) for k in range(nodes[i-1] + 1)]\n",
    "              for j in range(nodes[i])]\n",
    "        weights.append(np.matrix(w))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8abd82318ec6d8bd41f9faf047d07820772bb30d"
   },
   "source": [
    "**Forward Propagation:**\n",
    "\n",
    "* Each layer receives an input and computes an output. The output is computed by first calculating the dot product between the input and the weights of the layer and then passing this dot product through an activation function (in this case, the sigmoid function).\n",
    "* The output of each layer is the input of the next.\n",
    "* The input of the first layer is the feature vector.\n",
    "* The output of the final layer is the prediction of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "8d5a4637b25e338a583a86b615769e9d1ffe3faf"
   },
   "outputs": [],
   "source": [
    "def ForwardPropagation(x, weights, layers):\n",
    "    activations, layer_input = [x], x\n",
    "    for j in range(layers):\n",
    "        activation = Sigmoid(np.dot(layer_input, weights[j].T))\n",
    "        activations.append(activation)\n",
    "        layer_input = np.append(1, activation) # Augment with bias\n",
    "    \n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bf8542e13509a70418dc4dd1ebc9037e836fa373"
   },
   "source": [
    "**Backward Propagation:**\n",
    "\n",
    "* Calculate error at final output.\n",
    "* Propagate error backwards through the layers and perform corrections.\n",
    "    * Calculate Delta: Back-propagated error of current layer *times* Sigmoid derivation of current layer activation.\n",
    "    * Update Weights between current layer and previous layer: Multiply delta with activation of previous layer and learning rate, and add this product to weights of previous layer.\n",
    "    * Calculate error for current layer. Remove the bias from the weights of the previous layer and multiply the result with delta to get error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "da75ccf0d20cb7ad85d983147590de98987138c9"
   },
   "outputs": [],
   "source": [
    "def BackPropagation(y, activations, weights, layers):\n",
    "    outputFinal = activations[-1]\n",
    "    error = np.matrix(y - outputFinal) # Error at output\n",
    "    \n",
    "    for j in range(layers, 0, -1):\n",
    "        currActivation = activations[j]\n",
    "        \n",
    "        if(j > 1):\n",
    "            # Augment previous activation\n",
    "            prevActivation = np.append(1, activations[j-1])\n",
    "        else:\n",
    "            # First hidden layer, prevActivation is input (without bias)\n",
    "            prevActivation = activations[0]\n",
    "        \n",
    "        delta = np.multiply(error, SigmoidDerivative(currActivation))\n",
    "        weights[j-1] += lr * np.multiply(delta.T, prevActivation)\n",
    "\n",
    "        w = np.delete(weights[j-1], [0], axis=1) # Remove bias from weights\n",
    "        error = np.dot(delta, w) # Calculate error for current layer\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ffdc1c094d28243adae70b159696fd2b9bd7391e"
   },
   "source": [
    "In our implementation we will pass each sample of our dataset through the network, performing first the forward pass and then the weight updating via the back-propagation algorithm. Finally, the newly calculated weights will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d6bfd8a8c4638f24d2088766b5c0355abb7554aa"
   },
   "outputs": [],
   "source": [
    "def Train(X, Y, lr, weights):\n",
    "    layers = len(weights)\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], Y[i]\n",
    "        x = np.matrix(np.append(1, x)) # Augment feature vector\n",
    "        \n",
    "        activations = ForwardPropagation(x, weights, layers)\n",
    "        weights = BackPropagation(y, activations, weights, layers)\n",
    "\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "33e6098f976920c7dc734428d64f1c7b3d1c527d"
   },
   "outputs": [],
   "source": [
    "def Sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def SigmoidDerivative(x):\n",
    "    return np.multiply(x, 1-x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2117150dedc7833f562b229533c133a73e3e7a37"
   },
   "source": [
    "When we want to make a prediction for an item, we need to first pass it through the network. The output of the network (in the case of three different classes, as in the Iris problem) will be in the form `[x, y, z]` where `x, y, z` are real numbers in the range [0, 1]. The higher the value of an element, the more confident the network is that it is the correct class. We need to convert this output to the proper one-hot format we mentioned earlier. Thus, we will take the largest of the outputs and set the corresponding index to 1, while the rest are set to 0. This means the predicted class is the one the network is most confident in (ie. the greatest value).\n",
    "\n",
    "So, a prediction involves the forward propagation and the conversion of the output to one-hot encoding, with the 1 denoting the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "14567c4d692cd0beeae612b4b7f0e415c43c488e"
   },
   "outputs": [],
   "source": [
    "def Predict(item, weights):\n",
    "    layers = len(weights)\n",
    "    item = np.append(1, item) # Augment feature vector\n",
    "    \n",
    "    ##_Forward Propagation_##\n",
    "    activations = ForwardPropagation(item, weights, layers)\n",
    "    \n",
    "    outputFinal = activations[-1].A1\n",
    "    index = FindMaxActivation(outputFinal)\n",
    "\n",
    "    # Initialize prediction vector to zeros\n",
    "    y = [0 for i in range(len(outputFinal))]\n",
    "    y[index] = 1  # Set guessed class to 1\n",
    "\n",
    "    return y # Return prediction vector\n",
    "\n",
    "\n",
    "def FindMaxActivation(output):\n",
    "    \"\"\"Find max activation in output\"\"\"\n",
    "    m, index = output[0], 0\n",
    "    for i in range(1, len(output)):\n",
    "        if(output[i] > m):\n",
    "            m, index = output[i], i\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a62a105187489e3d61a5cdfb1b9b85c4604c48e4"
   },
   "source": [
    "Finally, we need a way to evaluate our network. For this, we will write the `Accuracy` function which, given the computed weights, predicts the class of each object in its input and checks it against the actual class, returning the percentage of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "1f772bccc2c489128b85f1de456539d51f76e611"
   },
   "outputs": [],
   "source": [
    "def Accuracy(X, Y, weights):\n",
    "    \"\"\"Run set through network, find overall accuracy\"\"\"\n",
    "    correct = 0\n",
    "\n",
    "    for i in range(len(X)):\n",
    "        x, y = X[i], list(Y[i])\n",
    "        guess = Predict(x, weights)\n",
    "\n",
    "        if(y == guess):\n",
    "            # Guessed correctly\n",
    "            correct += 1\n",
    "\n",
    "    return correct / len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8fd20cb4b503159525be7b0737fecc7f93b91539"
   },
   "source": [
    "We have now completed our implementation and we can check the results! Below we build a network by passing to the main function (`NeuralNetwork`) the training/validation sets, the number of epochs, the learning rate and the number of nodes in each layer.\n",
    "\n",
    "During the training, after each 20th epoch, the accuracy of the network on the training and validation sets will be printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "22bbc9e18ff2d48e7b71dc6b14b275782aa5c000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20\n",
      "Training Accuracy:0.7368421052631579\n",
      "Validation Accuracy:0.7692307692307693\n",
      "Epoch 40\n",
      "Training Accuracy:0.9649122807017544\n",
      "Validation Accuracy:1.0\n",
      "Epoch 60\n",
      "Training Accuracy:0.9473684210526315\n",
      "Validation Accuracy:0.9230769230769231\n",
      "Epoch 80\n",
      "Training Accuracy:0.9649122807017544\n",
      "Validation Accuracy:0.9230769230769231\n",
      "Epoch 100\n",
      "Training Accuracy:0.9649122807017544\n",
      "Validation Accuracy:0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "f = len(X[0]) # Number of features\n",
    "o = len(Y[0]) # Number of outputs / classes\n",
    "\n",
    "layers = [f, 5, 10, o] # Number of nodes in layers\n",
    "lr, epochs = 0.15, 100\n",
    "\n",
    "weights = NeuralNetwork(X_train, Y_train, X_val, Y_val, epochs=epochs, nodes=layers, lr=lr);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "beaa50682a3457ccd8df5d7185d6015d051712d5"
   },
   "source": [
    "For the grand finale, we will test the network against the testing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c6b37c7a6397f5b88cd538f63faea4c0969326df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.9565217391304348\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy: {}\".format(Accuracy(X_test, Y_test, weights)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
